Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	align
	1	all
	1	ancestral
	1	export
	1	filter
	1	parse
	1	refine
	1	translate
	1	tree
	9

[Sat May  2 15:18:23 2020]
Job 3: Parsing fasta into sequences and metadata

[Sat May  2 15:18:28 2020]
Finished job 3.
1 of 9 steps (11%) done

[Sat May  2 15:18:28 2020]
Job 9: 
        Filtering to
          - 5000 sequence(s) per country
          - minimum genome length of 1000
        

[Sat May  2 15:18:32 2020]
Error in rule filter:
    jobid: 9
    output: results/filtered_oc43_full.fasta
    shell:
        
        augur filter             --sequences results/sequences_oc43_full.fasta             --metadata results/metadata_oc43_full.tsv             --output results/filtered_oc43_full.fasta             --group-by country             --sequences-per-group 5000             --min-length 1000
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /Users/katekistler/nextstrain/seasonal-cov/oc43/.snakemake/log/2020-05-02T151823.048337.snakemake.log
