Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	align
	1	all
	1	ancestral
	1	export
	1	filter
	1	parse
	1	refine
	1	translate
	1	tree
	9

[Thu Apr 16 08:34:02 2020]
Job 3: Parsing fasta into sequences and metadata

[Thu Apr 16 08:34:04 2020]
Finished job 3.
1 of 9 steps (11%) done

[Thu Apr 16 08:34:04 2020]
Job 9: 
        Filtering to
          - 5000 sequence(s) per country
          - minimum genome length of 1000
        

[Thu Apr 16 08:34:06 2020]
Error in rule filter:
    jobid: 9
    output: results/filtered_cov_full.fasta
    shell:
        
        augur filter             --sequences results/sequences_cov_full.fasta             --metadata results/metadata_cov_full.tsv             --output results/filtered_cov_full.fasta             --group-by country             --sequences-per-group 5000             --min-length 1000
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /Users/katekistler/nextstrain/seasonal-cov/.snakemake/log/2020-04-16T083401.985191.snakemake.log
